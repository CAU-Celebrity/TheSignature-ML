{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "font transfer(tensor 변환 error).ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kxvCvbhwX6j",
        "outputId": "4f3fd588-2368-4224-8f4f-9e7ea1298e43"
      },
      "source": [
        "pip uninstall tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QgOJGJmwcl8",
        "outputId": "649003c7-c8d5-455a-b711-8d4403bea7c6"
      },
      "source": [
        "pip uninstall numpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling numpy-1.19.5:\n",
            "  Would remove:\n",
            "    /usr/bin/f2py\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.7\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy-1.19.5.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libgfortran-2e0d59d6.so.5.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libopenblasp-r0-09e95953.3.13.so\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libquadmath-2d0c479f.so.0.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libz-eb09ad1d.so.1.2.3\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled numpy-1.19.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "PtnFyETjq1LP",
        "outputId": "3ab7fe62-c39c-463b-bfaa-4600dee1d0ab"
      },
      "source": [
        "pip install numpy==1.19"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/31/e2c3eda7afe7dab08e1f24767b8e38ff2f30dc82bd74aa3a5324c550366a/numpy-1.19.0-cp37-cp37m-manylinux2010_x86_64.whl (14.6MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6MB 5.4MB/s \n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.19.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFam5zd1wbpy",
        "outputId": "f8293cdd-ff87-496b-cc92-839eebff07b0"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 58.8MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=83705bede846351cf05ad39b4140795686742807356c86685a79f4be6cd2d542\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2dNpZMzjuGY",
        "outputId": "98aef0a4-4b2a-42e6-8164-16a607899f68"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "import json\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "# from keras.datasets import mnist\n",
        "# from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "# from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "# from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "# from keras.models import Sequential, Model\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pgcdvYrxrA65",
        "outputId": "1e96100c-96a0-4ee8-b0a0-a9fc36cbc07f"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "455WdSw5s9b6",
        "outputId": "330113d0-fd4a-4b75-e3de-5b597c06d7d4"
      },
      "source": [
        "np.__version__"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.19.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFBtAIHJnR8h"
      },
      "source": [
        "def crop_image(img):\n",
        "    image = np.array(img)\n",
        "    blur = cv2.GaussianBlur(image, ksize=(3,3), sigmaX=0)\n",
        "    ret, thresh1 = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY)\n",
        "    edged = cv2.Canny(blur, 10, 250)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\n",
        "    closed = cv2.morphologyEx(edged, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(closed.copy(),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    total = 0\n",
        "    contours_xy = np.array(contours)\n",
        "    x_min, x_max = 0,0\n",
        "    value = list()\n",
        "    for i in range(len(contours_xy)):\n",
        "      for j in range(len(contours_xy[i])):\n",
        "        value.append(contours_xy[i][j][0][0])\n",
        "        x_min = min(value)\n",
        "        x_max = max(value)\n",
        " \n",
        "    y_min, y_max = 0,0\n",
        "    value = list()\n",
        "    for i in range(len(contours_xy)):\n",
        "      for j in range(len(contours_xy[i])):\n",
        "        value.append(contours_xy[i][j][0][1])\n",
        "        y_min = min(value)\n",
        "        y_max = max(value)\n",
        "\n",
        "    x = x_min\n",
        "    y = y_min\n",
        "    w = x_max-x_min\n",
        "    h = y_max-y_min\n",
        "\n",
        "    return x, y, w, h\n",
        "\n",
        "def process_image(img, x, y, w, h, canvas_size):\n",
        "    new_width = 50\n",
        "    new_height = int(50 * h / w)\n",
        "    if new_height > 60:\n",
        "        new_height = 55\n",
        "        new_width = int(55 * w / h)\n",
        "    img = img.crop((x-1, y-1, x+w+1, y+h+1)).resize((new_width,new_height))\n",
        "    new_left = int((64 - img.width) / 2)\n",
        "    new_top = int((64 - img.height) / 2)\n",
        "    result = Image.new(\"L\", (canvas_size, canvas_size), color=255)\n",
        "    result.paste(img, (new_left, new_top))\n",
        "\n",
        "    return result\n",
        "\n",
        "def draw_single_char(ch, font, canvas_size, x_offset, y_offset):\n",
        "    img = Image.new(\"L\", (150, 150), color=255)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    # draw.text((x_offset, y_offset), ch, fill=(0), font=font, stroke_width=2, stroke_fill=\"black\")\n",
        "    draw.text((x_offset, y_offset), ch, fill=(0), font=font)\n",
        "    x, y, w, h = crop_image(img)\n",
        "    img = process_image(img, x, y, w, h, canvas_size)\n",
        "    return img\n",
        "\n",
        "\n",
        "def font2img(ttf, charset, char_size, canvas_size, x_offset, y_offset, folder):\n",
        "    global X_train, y_train\n",
        "    font = ImageFont.truetype(ttf, size=char_size)\n",
        "    count = 0\n",
        "    for c in charset:\n",
        "        e = draw_single_char(c, font, canvas_size, x_offset, y_offset)\n",
        "        if e:\n",
        "            e.save(folder+c+'.png')\n",
        "            # X_train.append(np.array(e))\n",
        "            # y_train.append(count)\n",
        "            count += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwje8WkTncQs"
      },
      "source": [
        "path = './gdrive/MyDrive/hangul/ttf/'\n",
        "fonts = os.listdir(path)\n",
        "charset = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "char_size, canvas_size = 100, 64\n",
        "x_offset, y_offset = 10, 10\n",
        "label = 1\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "srcfont = fonts[0]\n",
        "font2img(path + srcfont, charset, char_size, canvas_size, x_offset, y_offset, 'src/')\n",
        "\n",
        "dstfont = fonts[1]\n",
        "font2img(path + dstfont, charset, char_size, canvas_size, x_offset, y_offset, 'dst/')\n",
        "\n",
        "# for font in fonts:\n",
        "#     font2img(path + font, charset, char_size, canvas_size, x_offset, y_offset)\n",
        "#     print('font#%02d 데이터셋 생성 완료' % (label))\n",
        "#     label += 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITnTIglIp3ZG",
        "outputId": "6bb2760b-0e4a-41c9-adb2-9729a72b6941"
      },
      "source": [
        "np.array(Image.open('src/A.png')).shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gv62ONJ696y"
      },
      "source": [
        "# encoding=UTF-8\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "# %%\n",
        "def data_preprocess(path='',\n",
        "                    source_font='src',\n",
        "                    target_font='dst'):\n",
        "    \"\"\"\n",
        "        normalize font images to [0,1] and then create the source-target font pairs\n",
        "    \"\"\"\n",
        "    # load the source fonts and target fonts\n",
        "    source_path = source_font #path+'/'+source_font\n",
        "    source_fonts = list()\n",
        "    target_path = target_font #path + '/' + target_font\n",
        "    target_fonts = list()\n",
        "    files = os.listdir(source_path)\n",
        "    mid1 = np.zeros((64, 64, 1))\n",
        "    mid2 = np.zeros((64, 64, 1))   \n",
        "\n",
        "    \n",
        "    for file in files:\n",
        "        #load a source font\n",
        "        img1 = cv2.imread(source_path+'/'+file) #Image.open(source_path+'/'+file)\n",
        "        np1 = np.array(img1)\n",
        "        mid1 = np1[:, :, 0]\n",
        "        mid1 = mid1[:,:,np.newaxis]\n",
        "        source_fonts.append(mid1)\n",
        "\n",
        "        #load a target font\n",
        "        img2 = cv2.imread(source_path+'/'+file) #Image.open(target_path + '/'+file)\n",
        "        np2 = np.array(img2)\n",
        "        mid2 = np2[:, :, 0]\n",
        "        mid2 = mid2[:,:,np.newaxis]\n",
        "        target_fonts.append(mid2)\n",
        "\n",
        "    source_fonts = np.array(source_fonts)\n",
        "    source_fonts = source_fonts.astype(np.float32)\n",
        "    source_fonts /= 255\n",
        "\n",
        "    target_fonts = np.array(target_fonts)\n",
        "    target_fonts = target_fonts.astype(np.float32)\n",
        "    target_fonts /= 255\n",
        "\n",
        "\n",
        "    return source_fonts, target_fonts\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, source_fonts, target_fonts, test_frac=0.4, val_frac=0.1, scale_func=None):\n",
        "        \"\"\"\n",
        "            create the training set, validation set and testing set\n",
        "        \"\"\"\n",
        "        self.data_num = int(len(source_fonts))\n",
        "        self.val_num = int(self.data_num*(1-test_frac)*val_frac)\n",
        "        self.train_num = int(self.data_num * (1 - test_frac)) - self.val_num\n",
        "        self.test_num = self.data_num-self.val_num-self.train_num\n",
        "\n",
        "        self.train = {}\n",
        "        self.test = {}\n",
        "        self.valid = {}\n",
        "\n",
        "\n",
        "        \"\"\"self.train['source_font'] = np.rollaxis(\n",
        "            source_fonts[:self.train_num], axis=3)\n",
        "        self.valid['source_font'] = np.rollaxis(\n",
        "            source_fonts[self.train_num: self.train_num+self.val_num], axis=3)\n",
        "        self.test['source_font'] = np.rollaxis(\n",
        "            source_fonts[self.train_num + self.val_num:], axis=3)\n",
        "        self.train['target_font'] = np.rollaxis(\n",
        "            target_fonts[:self.train_num], axis=3)\n",
        "        self.valid['target_font'] = np.rollaxis(\n",
        "            target_fonts[self.train_num: self.train_num+self.val_num], axis=3)\n",
        "        self.test['target_font'] = np.rollaxis(\n",
        "            target_fonts[self.train_num + self.val_num:], axis=3)\"\"\"\n",
        "\n",
        "        self.train['source_font'] =source_fonts[:self.train_num]\n",
        "        self.valid['source_font'] =source_fonts[self.train_num: self.train_num+self.val_num]\n",
        "        self.test['source_font'] = source_fonts[self.train_num + self.val_num:]\n",
        "\n",
        "        self.train['target_font'] =target_fonts[:self.train_num]\n",
        "        self.valid['target_font'] =target_fonts[self.train_num: self.train_num+self.val_num]\n",
        "        self.test['target_font'] =target_fonts[self.train_num + self.val_num:]\n",
        "\n",
        "    def shuffle_data( self ):\n",
        "        idx = np.arange(self.train_num)\n",
        "        np.random.shuffle(idx)\n",
        "        self.train['source_font'], self.train['target_font'] = self.train['source_font'][idx], self.train['target_font'][idx]\n",
        "\n",
        "    def get_batches(self, batch_size):\n",
        "        \"\"\"\n",
        "            generate one batch of data\n",
        "        \"\"\"\n",
        "        batch_num = self.train_num // batch_size\n",
        "\n",
        "        for ii in range(0, batch_num * batch_size, batch_size):\n",
        "            source_font = self.train['source_font'][ii:ii+batch_size]\n",
        "            target_font = self.train['target_font'][ii:ii+batch_size]\n",
        "\n",
        "            yield source_font, target_font\n",
        "\n",
        "\n",
        "# %%\n",
        "#source_fonts, target_fonts = data_preprocess()\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "#dataset = Dataset( source_fonts, target_fonts)\n",
        "#print(dataset.train['source_font'].shape)\n",
        "\n",
        "# 解决getbatches报错：'KeyValueError target_font'\n",
        "# %%"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "RwXztEpsAFGw",
        "outputId": "f00fa51b-40d1-4235-8e44-842347546949"
      },
      "source": [
        "  \n",
        "# %%\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "# %%\n",
        "# hyperparameters\n",
        "image_dim = (64, 64, 1)\n",
        "learning_rate = 0.005\n",
        "batch_size = 32\n",
        "element_num = batch_size*image_dim[0]*image_dim[1]*image_dim[2]\n",
        "epochs = 30\n",
        "beta1 = 0.9\n",
        "lambda_p = 1\n",
        "lambda_1 = 10\n",
        "lambda_2 = 1\n",
        "lambda_3 = 1\n",
        "lambda_4 = 30\n",
        "print_every = 1\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "def leaky_relu(x):\n",
        "    alpha = -0.1\n",
        "    return tf.where( x >= 0.0, x, alpha*x )\n",
        "\n",
        "def selu(x):\n",
        "    alpha = 1.6732632423543772848170429916717\n",
        "    scale = 1.0507009873554804934193349852946\n",
        "    return scale*tf.where(x >= 0.0, x, alpha*tf.nn.elu(x))\n",
        "\n",
        "\n",
        "# %%\n",
        "def generator(source_font, reuse=False, training=True):\n",
        "\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "        # input tensor size is 64*64*1\n",
        "        input_layer = tf.reshape(source_font, (-1, 64, 64, 1))\n",
        "\n",
        "        # 64*64*1 -> 64*64*64\n",
        "        conv1 = tf.layers.conv2d(\n",
        "            inputs=input_layer, filters=64, kernel_size=3, padding='same')\n",
        "        conv1 = tf.layers.batch_normalization(conv1, training=True)\n",
        "        conv1 = leaky_relu(conv1)\n",
        "\n",
        "        # 128*128*64 -> 64*64*128\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            inputs=conv1, filters=64, kernel_size=3, strides=2, padding='same')\n",
        "        conv2 = tf.layers.batch_normalization(conv2, training=True)\n",
        "        conv2 = leaky_relu(conv2)\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            inputs=conv2, filters=128, kernel_size=3, padding='same')\n",
        "        conv2 = tf.layers.batch_normalization(conv2, training=True)\n",
        "        conv2 = leaky_relu(conv2)\n",
        "\n",
        "        # 64*64*128 -> 32*32*256\n",
        "        conv3 = tf.layers.conv2d(\n",
        "            inputs=conv2, filters=128, kernel_size=3, strides=2, padding='same')\n",
        "        conv3 = tf.layers.batch_normalization(conv3, training=True)\n",
        "        conv3 = leaky_relu(conv3)\n",
        "        conv3 = tf.layers.conv2d(\n",
        "            inputs=conv3, filters=256, kernel_size=3, padding='same')\n",
        "        conv3 = tf.layers.batch_normalization(conv3, training=True)\n",
        "        conv3 = leaky_relu(conv3)\n",
        "\n",
        "        # 32*32*256 -> 16*16*512\n",
        "        conv4 = tf.layers.conv2d(\n",
        "            inputs=conv3, filters=256, kernel_size=3, strides=2, padding='same')\n",
        "        conv4 = tf.layers.batch_normalization(conv4, training=True)\n",
        "        conv4 = leaky_relu(conv4)\n",
        "        conv4 = tf.layers.conv2d(\n",
        "            inputs=conv4, filters=512, kernel_size=3, padding='same')\n",
        "        conv4 = tf.layers.batch_normalization(conv4, training=True)\n",
        "        conv4 = leaky_relu(conv4)\n",
        "\n",
        "        # 16*16*512 -> 8*8*512\n",
        "        conv5 = tf.layers.conv2d(\n",
        "            inputs=conv4, filters=512, kernel_size=3, strides=2, padding='same')\n",
        "        conv5 = tf.layers.batch_normalization(conv5, training=True)\n",
        "        conv5 = leaky_relu(conv5)\n",
        "\n",
        "        # 8*8*512 -> 16*16*512\n",
        "        # 16*16*512 + 16*16*512 -> 16*16*1024\n",
        "        # 16*16*1024 -> 16*16*512\n",
        "        # 16*16*512 -> 16*16*256\n",
        "        up6 = tf.layers.conv2d_transpose(\n",
        "            inputs=conv5, filters=512, kernel_size=3, strides=2, padding='same')\n",
        "        up6 = tf.layers.batch_normalization(up6, training=True)\n",
        "        up6 = leaky_relu(up6)\n",
        "        up6 = tf.concat([conv4, up6], axis=3)\n",
        "        conv6 = tf.layers.conv2d(\n",
        "            inputs=up6, filters=512, kernel_size=3, padding='same')\n",
        "        conv6 = tf.layers.batch_normalization(conv6, training=True)\n",
        "        conv6 = leaky_relu(conv6)\n",
        "        conv6 = tf.layers.conv2d(\n",
        "            inputs=conv6, filters=256, kernel_size=3, padding='same')\n",
        "        conv6 = tf.layers.batch_normalization(conv6, training=True)\n",
        "        conv6 = leaky_relu(conv6)\n",
        "\n",
        "        # 16*16*256 -> 32*32*256\n",
        "        # 32*32*256 + 32*32*256 -> 32*32*512\n",
        "        # 32*32*512 -> 32*32*256\n",
        "        # 32*32*256 -> 32*32*128\n",
        "        up7 = tf.layers.conv2d_transpose(\n",
        "            inputs=conv6, filters=256, kernel_size=3, strides=2, padding='same')\n",
        "        up7 = tf.layers.batch_normalization(up7, training=True)\n",
        "        up7 = leaky_relu(up7)\n",
        "        up7 = tf.concat([conv3, up7], axis=3)\n",
        "        conv7 = tf.layers.conv2d(\n",
        "            inputs=up7, filters=256, kernel_size=3, padding='same')\n",
        "        conv7 = tf.layers.batch_normalization(conv7, training=True)\n",
        "        conv7 = leaky_relu(conv7)\n",
        "        conv7 = tf.layers.conv2d(\n",
        "            inputs=conv7, filters=128, kernel_size=3, padding='same')\n",
        "        conv7 = tf.layers.batch_normalization(conv7, training=True)\n",
        "        conv7 = leaky_relu(conv7)\n",
        "\n",
        "        # 32*32*128 -> 64*64*128\n",
        "        # 64*64*128 + 64*64*128 -> 64*64*256\n",
        "        # 64*64*256 -> 64*64*128 -> 64*64*64\n",
        "        up8 = tf.layers.conv2d_transpose(\n",
        "            inputs=conv7, filters=128, kernel_size=3, strides=2, padding='same')\n",
        "        up8 = tf.layers.batch_normalization(up8, training=True)\n",
        "        up8 = leaky_relu(up8)\n",
        "        up8 = tf.concat([conv2, up8], axis=3)\n",
        "        conv8 = tf.layers.conv2d(\n",
        "            inputs=up8, filters=128, kernel_size=3, padding='same')\n",
        "        conv8 = tf.layers.batch_normalization(conv8, training=True)\n",
        "        conv8 = leaky_relu(conv8)\n",
        "        conv8 = tf.layers.conv2d(\n",
        "            inputs=conv8, filters=64, kernel_size=3, padding='same')\n",
        "        conv8 = tf.layers.batch_normalization(conv8, training=True)\n",
        "        conv8 = leaky_relu(conv8)\n",
        "\n",
        "        # 64*64*64 -> 128*128*64\n",
        "        # 128*128*64 + 128*128*64 -> 128*128*128\n",
        "        # 128*128*128 -> 128*128*64 -> 128*128*1\n",
        "        up9 = tf.layers.conv2d_transpose(\n",
        "            inputs=conv8, filters=64, kernel_size=3, strides=2, padding='same')\n",
        "        up9 = tf.layers.batch_normalization(up9, training=True)\n",
        "        up9 = leaky_relu(up9)\n",
        "        up9 = tf.concat([conv1, up9], axis=3)\n",
        "        conv9 = tf.layers.conv2d(\n",
        "            inputs=up9, filters=64, kernel_size=3, padding='same')\n",
        "        conv9 = tf.layers.batch_normalization(conv9, training=True)\n",
        "        conv9 = leaky_relu(conv9)\n",
        "        conv9 = tf.layers.conv2d(\n",
        "            inputs=conv9, filters=1, kernel_size=3, padding='same')\n",
        "        conv9 = tf.layers.batch_normalization(conv9, training=True)\n",
        "        conv9 = tf.nn.sigmoid(conv9)\n",
        "\n",
        "        return conv9\n",
        "\n",
        "\n",
        "# %%\n",
        "def discriminator(x, reuse=False):\n",
        "    \"\"\"\n",
        "        x: a real image of target fonts or a fake image generated by generator use source fonts\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "        w_init = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "        conv1 = tf.layers.conv2d(\n",
        "            inputs=x, filters=256, kernel_size=3, strides=2, padding='same')\n",
        "        conv1 = tf.layers.batch_normalization(conv1, training=True)\n",
        "        conv1 = leaky_relu(conv1)\n",
        "\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            inputs=conv1, filters=512, kernel_size=3, strides=2, padding='same')\n",
        "        conv2 = tf.layers.batch_normalization(conv2, training=True)\n",
        "        conv2 = leaky_relu(conv2)\n",
        "\n",
        "        conv3 = tf.layers.conv2d(\n",
        "            inputs=conv2, filters=1024, kernel_size=3, strides=2, padding='same')\n",
        "        conv3 = tf.layers.batch_normalization(conv3, training=True)\n",
        "        conv3 = leaky_relu(conv3)\n",
        "\n",
        "        conv4 = tf.layers.conv2d(\n",
        "            inputs=conv3, filters=2048, kernel_size=3, strides=2, padding='same')\n",
        "        conv4 = tf.layers.batch_normalization(conv4, training=True)\n",
        "        conv4 = leaky_relu(conv4)\n",
        "\n",
        "        flat = tf.reshape(conv4, (-1, 8*8*2048))\n",
        "        logits = tf.layers.dense(flat, 1)\n",
        "        out = tf.nn.sigmoid(logits)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# %%\n",
        "class TransNN:\n",
        "    def __init__(self):\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "    '''def input_setup(self, image_dim):\n",
        "        \n",
        "        self.target_fonts = tf.placeholder(\n",
        "            tf.float32, (None,  128, 128, 1), name='target_font')\n",
        "        self.source_fonts = tf.placeholder(\n",
        "            tf.float32, (None,  128, 128, 1), name='source_font')'''\n",
        "\n",
        "    def model_setup(self):\n",
        "        '''\n",
        "        This function sets up the model to train\n",
        "        '''\n",
        "        self.target_fonts = tf.placeholder(\n",
        "            tf.float32, (None,  64, 64, 1), name='target_fonts')\n",
        "        self.source_fonts = tf.placeholder(\n",
        "            tf.float32, (None,  64, 64, 1), name='source_fonts')\n",
        "\n",
        "        self.generated_fonts = generator(self.source_fonts)\n",
        "        self.real_score = discriminator(self.target_fonts)\n",
        "        self.fake_score = discriminator(self.generated_fonts, reuse=True)\n",
        "\n",
        "    def model_loss(self):\n",
        "        #\bpix2pix cross_entropy loss\n",
        "        p2p_cross_entropy = tf.reduce_sum(- lambda_p * np.multiply(self.target_fonts, tf.log(self.generated_fonts)\n",
        "                                                         ) - np.multiply(1-self.target_fonts, tf.log(1-self.generated_fonts)))\n",
        "        diffrence = tf.subtract(self.target_fonts, self.generated_fonts)\n",
        "        #l1-norm\n",
        "        l1_norm = tf.reduce_sum( tf.abs(diffrence))\n",
        "        #l2-norm\n",
        "        l2_norm = tf.reduce_sum( np.multiply( diffrence, diffrence))\n",
        "\n",
        "        v1_loss = lambda_1 * p2p_cross_entropy + lambda_4 * l1_norm \n",
        "        v1_loss /= element_num\n",
        "        #v1_loss = v1_loss / target_font.shape[0]\n",
        "\n",
        "        # v2_loss evaluates the discriminator performence of seperating generated fonts from target fonts\n",
        "        v2_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_score, labels=tf.ones_like(\n",
        "            self.real_score))) + tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_score, labels=tf.zeros_like(self.fake_score)))\n",
        "\n",
        "        # v3_loss evaluate the generator performence of generating real fonts\n",
        "        v3_loss_log = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=self.fake_score, labels=tf.ones_like(self.fake_score)))\n",
        "        # in case of gradient varnishment, we use v3_loss without log\n",
        "        v3_loss = tf.reduce_sum(self.fake_score)\n",
        "\n",
        "\n",
        "        #self.generator_loss =  ( lambda_1 * v1_loss + lambda_2 * v3_loss_log ) / batch_size\n",
        "        #self.generator_loss =  ( lambda_1 * v1_loss + lambda_2 *( batch_size -  v3_loss ) ) / batch_size\n",
        "        self.generator_loss =  v1_loss  / batch_size \n",
        "\n",
        "        self.discriminator_loss = ( lambda_3 * v2_loss ) / batch_size\n",
        "\n",
        "        self.model_vars = tf.trainable_variables()\n",
        "\n",
        "        d_vars = [var for var in self.model_vars if var.name.startswith(\n",
        "            'discriminator')]\n",
        "        g_vars = [\n",
        "            var for var in self.model_vars if var.name.startswith('generator')]\n",
        "\n",
        "        self.d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(\n",
        "            self.discriminator_loss, var_list=d_vars)\n",
        "        self.g_train_opt = tf.train.AdamOptimizer(\n",
        "            learning_rate, beta1=beta1).minimize(self.generator_loss, var_list=g_vars)\n",
        "\n",
        "    def train(self):\n",
        "        self.model_setup()\n",
        "\n",
        "        self.model_loss()\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.Session(config=config) as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            # Training\n",
        "            for epoch_no in range(epochs):\n",
        "                dataset.shuffle_data()\n",
        "\n",
        "                batch_no = 0\n",
        "                for source_font, target_font in dataset.get_batches(batch_size):\n",
        "                    batch_no += 1\n",
        "                    print(\"Batch No: {}/{} \".format(batch_no,\n",
        "                                                    dataset.train_num // batch_size))\n",
        "\n",
        "                    # Use generator to generate fonts from source fonts\n",
        "                    sess.run(self.generated_fonts, feed_dict={\n",
        "                             self.source_fonts: source_font, self.target_fonts: target_font})\n",
        "\n",
        "                    # Optimize the discriminator\n",
        "                    sess.run(self.d_train_opt, feed_dict={self.target_fonts: target_font, self.source_fonts: source_font})\n",
        "\n",
        "                    # Optimize the generator\n",
        "                    sess.run(self.g_train_opt, feed_dict={\n",
        "                             self.target_fonts: target_font, self.source_fonts: source_font})\n",
        "\n",
        "                    if epoch_no % print_every == 0:\n",
        "                        train_loss_d = self.discriminator_loss.eval(\n",
        "                            {self.target_fonts: target_font, self.source_fonts: source_font})\n",
        "                        train_loss_g = self.generator_loss.eval(\n",
        "                            {self.source_fonts: source_font, self.target_fonts: target_font})\n",
        "\n",
        "                        print(\"Epoch {}/{}...\".format(epoch_no+1, epochs),\n",
        "                              \"Discriminator Loss: {:.8f}...\".format(\n",
        "                                  train_loss_d),\n",
        "                              \"Generator Loss: {:.8f}\".format(train_loss_g))\n",
        "                        # Save losses to view after training\n",
        "                        #losses.append((train_loss_d, train_loss_g))\n",
        "\n",
        "            saver.save(sess, './checkpoints/generator')\n",
        "\n",
        "    def test(self):\n",
        "        print(\"Testing the results\")\n",
        "\n",
        "        # self.model_setup()\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.Session(config=config) as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            saver = tf.train.import_meta_graph('./checkpoints/generator.meta')\n",
        "            saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n",
        "            graph = tf.get_default_graph()\n",
        "            image_no = 0\n",
        "\n",
        "            for num in range(0, dataset.test_num, batch_size):\n",
        "                generated_fonts = sess.run(self.generated_fonts, feed_dict={\n",
        "                                           self.source_fonts: dataset.test['source_font'][num:num+batch_size], self.target_fonts: dataset.test['target_font'][num:num+batch_size]})\n",
        "                source_fonts = dataset.test['target_font'][num:num+batch_size]\n",
        "\n",
        "                for batch_no in range(min(batch_size, generated_fonts.shape[0])):\n",
        "                    generated_font = generated_fonts[batch_no]\n",
        "                    target_font = source_fonts[batch_no]\n",
        "\n",
        "                    mid = np.append(generated_font, generated_font, axis=2)\n",
        "                    mid = np.append(generated_font, mid, axis=2)\n",
        "                    mid = mid*255\n",
        "                    mid = mid.astype('uint8')\n",
        "                    img = Image.fromarray(mid)\n",
        "                    img.save(fp='./generated_fonts/generated_' +\n",
        "                             str(image_no) + '.jpg')\n",
        "\n",
        "                    mid[:, :, :] = target_font\n",
        "                    mid = mid*255\n",
        "                    mid = mid.astype('uint8')\n",
        "                    img = Image.fromarray(mid)\n",
        "                    img.save(fp='./generated_fonts/target_' +\n",
        "                             str(image_no)+'.jpg')\n",
        "\n",
        "                    image_no += 1\n",
        "\n",
        "\n",
        "# %%\n",
        "# writer=tf.summary.FileWriter(r\"/Users/hangyizhe/GitHub/Chinese_Font_Transfer/models\",tf.get_default_graph())\n",
        "# writer.close()\n",
        "source_fonts, target_fonts = data_preprocess()\n",
        "dataset = Dataset(source_fonts, target_fonts, test_frac=0.1, val_frac=0)\n",
        "\n",
        "# %%\n",
        "net = TransNN()\n",
        "net.train()\n",
        "print(\"Training Finished!\")\n",
        "\n",
        "net.test()\n",
        "\n",
        "print(\"Test Finished!\")\n",
        "\n",
        "\n",
        "# %%"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-092bafd7505d>:48: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-092bafd7505d>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-6-092bafd7505d>:31: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-6-092bafd7505d>:93: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-092bafd7505d>:190: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-092bafd7505d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-092bafd7505d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-092bafd7505d>\u001b[0m in \u001b[0;36mmodel_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m#\bpix2pix cross_entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       p2p_cross_entropy = tf.reduce_sum(- lambda_p * np.multiply(self.target_fonts, tf.log(self.generated_fonts)\n\u001b[0m\u001b[1;32m    224\u001b[0m                                                        ) - np.multiply(1-self.target_fonts, tf.log(1-self.generated_fonts)))\n\u001b[1;32m    225\u001b[0m       \u001b[0mdiffrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_fonts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated_fonts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (target_fonts:0) to a numpy array."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLldz3X1pSC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}